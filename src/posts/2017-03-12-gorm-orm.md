---
title: GORM orm
summary: Having a look at an Object-Relational Mapper for Go. Makes mapping Go data types over multiple back ends easy
categories: go orm gorm
---

Go fits a nieche in the app world where you're willing to trade off instant reach-anywhere flexibility for performance.
It doesn't have Ruby's object system but it is able to give you results in a fraction of the time.

Recently, I was tasked with writing a network service that was able to test a javascript binary on tens of thousands of sites. Managing external WebDriver sessions in the usual Ruby would be annoying as it's primatives for handling signals and programming asynchronously are annoying and too slow for the performance guarantees: stakeholders wanted to know within 5 minutes of a change to a client site that caused errors with the javascript binary. Whilst its syntax for calling out to the O/S is great, managing time outs and structuring the solution as a series of functions would be shit. So, I went with Go.

The experience was great. I was quickly able to use go-routines and channels to offer guarantees about observations on test results. The worst part was needing to implement a lot of functionality that users of ActiveRecord (or more sophisticated ORM's) take for granted. Here's a quick post showing a go package and it's companion test, walking through how to use `GORM` to switch out the persistence layer from `sqlite` to `posgresql` depending on the environment.

### orm

This package abstracted over serialization logic, connection handling, pooling, automated database migration, database selection and disposal. Whilst it seems like a lot for a larger application, the data this application generated was not really that important. It could be queried. It was serialized sanely. Don't really need too much engineering here

```go
package orm // I stuck all the code in an `orm` package

import (
    "fmt"
    "os"
    "sync"
    "time"

    "github.com/golang/glog" // Widely-used, flexible, tiered logging
    "github.com/jinzhu/gorm" //
    "github.com/maxwellforest/onefill/db/gorm/runner/pg"
    "github.com/maxwellforest/onefill/db/gorm/runner/sqlite"
)

/*
*
* RunRecords are product types consisting of a site that was tested, the log of results and an arbitrary primary key.
* gorm.Model provides an automatically increased integer primary key for efficiency but I also supplied a JobID as a UUID.
* It was never used. Let that be a lesson to ya.
*
*/

type RunRecord struct {
    gorm.Model
    Log    string
    SiteId string
    JobID  string
}

var (
    pg_cnf           *pg.Config
    migrated         bool
    connection_count = 2 // Maximum number of connections. After profiling, settled on 2.
    connections      = make(chan *gorm.DB, connection_count) // Queue for holding database connections
    initMutex        = &sync.Mutex{} // Mutex for preventing queries during migration
)

/*
*
* ORM type was just a wrapper around a gorm.DB connection with
* some additional application-specific methods for querying the database
*
*/

type ORM struct {
    DB *gorm.DB
}

/*
*
* Close was called to free the connection back into the pooling. The first approximation solution did not use connection pooling
* but allowed any number of goroutines (~10,000) to create database connections. This caused problems for
* integration tests once it came time for deployment as most databases consider 20 a large number of outstanding
* connections. As a result, we needed to explicitly manage the lifecycle of database connection by
* closing the connections. In the future, this should be implemented as a destructor on the ORM struct.
* This way, there wouldn't be a need for callers to expressly call a destructor method.
*
*/
func (o *ORM) Close() error {
    connections <- o.DB
    o.DB = nil
    return nil
}

/*
*
* In the future, this method could be optimized by accepting a default paging argument.
* This paging argument could be used to statically allocate space for the array of pointers to
* Run Records. Whilst early optimization is the blah blah blah, this is one quick way to
* speed things up.
*/
func (o *ORM) GetRunRecordsForSite(site_id *string) ([]*RunRecord, error) {
    var rr []*RunRecord
    if o.DB == nil {
        return nil, fmt.Errorf("DB connection not available")
    }
    o.DB.Where("site_id = ?", *site_id).Find(&rr)
    return rr, nil
}

func (o *ORM) GetTotalRunRecords() (int, error) {
    var count int
    if o.DB == nil {
        if o.DB == nil {
            return 0, fmt.Errorf("DB connection not available")
        }
    }
    o.DB.Table("run_records").Count(&count)
    return count, nil
}

func (o *ORM) GetRunRecord(run_record_id int) (*RunRecord, error) {
    var rr RunRecord
    if o.DB == nil {
        if o.DB == nil {
            return nil, fmt.Errorf("DB connection not available")
        }
    }

    o.DB.First(&rr, run_record_id)

    return &rr, nil
}

/*
*
* Again, should have resolved the limit here at compile time to statically allocate the
* array of RunRecord pointers. It's not required to be a flexible interface.
*
*/
func (o *ORM) GetRunRecords(page int, limit int) ([]*RunRecord, error) {
    var rr []*RunRecord
    if o.DB == nil {
        if o.DB == nil {
            return nil, fmt.Errorf("DB connection not available")
        }
    }
    offset := (page - 1) * limit
    o.DB.Limit(limit).Offset(offset).Find(&rr)
    return rr, nil
}

/*
*
* By taking pointers I minimized the cost of copying across activation records. The logs, after deserialized
* the tcp session, could frequently ammount to up to 2Kb.
*
*/

func (o *ORM) CreateRunRecordForSite(log *string, site_id *string, job_id *string) (*RunRecord, error) {
    if o.DB == nil {
        return nil, fmt.Errorf("DB connection not available")
    }

    rr := &RunRecord{Log: *log, SiteId: *site_id, JobID: *job_id}
    o.DB.Create(rr)
    return rr, nil
}

/*
*
* I feel like this method should lie in the pg-specific helper module `db/gorm/runner/pg`.
* It just calls through to it anyway.
* This method causes the `orm` module to retain the postgres configuration object.
* In Go there's no way to specify weak pointers.
* Most solutions look like a stack machine anyway so with reference counting on the
* heap, it's generally not a problem.
*
*/

func InitPG(cnf pg.Config) error {
    db, err := pg.Open(&cnf)
    if err != nil {
        return err
    }
    defer db.Close()

    pg_cnf = &cnf
    return nil
}


// initDB is called by each worker in the connection pool.
func initDB() (*gorm.DB, error) {
    var (
        db  *gorm.DB
        err error
    )
    // 1. Need to include a retry mechanism for failed initializations of the network
    tries := 5
    attempts := 0

    // 2. The module holds a mutex that allows one worker to create a connection at a time
    initMutex.Lock()
    defer initMutex.Unlock()

    for db == nil {
        // 3. If the pg configuration is nil, the worker uses the local sqlite store
        // this could be made more expressive
        if pg_cnf != nil {
            db, err = pg.Open(pg_cnf)
            if err != nil {
                return nil, err
            }
        } else {
            db, err = sqlite.Open(&sqlite.PathToDB)
        }
        if err != nil && attempts > tries {
            return nil, fmt.Errorf("Exceeded retries. Error: %s", err.Error())
        }
        attempts += 1
        time.Sleep(time.Second)
    }

    // 3. While holding the mutex, one goroutine is able to migrate the underlying db
    if db != nil && !migrated {
        db.AutoMigrate(&RunRecord{})
        migrated = true
    }

    return db, nil
}

/*
*
* This method is only called by the test suite. It is essentially a DB helper
* In the future, I'd probably move this to the test suite as it is not defined
* for the postgres backend
*
*/
func CleanUpDB(pathToDB string) error {
    err := os.Remove(pathToDB)
    return err
}

/*
*
* Implements connection pooling by taking a connection off the queue and creating
* a new ORM object. Goroutines will queue here waiting for the connection to become
* available
*
*/
func GetORMOrFail() (*ORM, error) {
    db := <-connections
    orm := &ORM{db}

    return orm, nil
}

/*
*
* Called at `init`, here is how we build the connection queue by
* using a fixed-sized channel or failing
*
*/
func InitConnections() {
    for i := connection_count; i > 0; i-- {
        db, err := initDB()
        if err != nil {
            glog.Fatalf("Failed to init connections: %s", err.Error())
        }
        connections <- db
    }
}
```

```go
package orm

import (
    "testing"

    "github.com/maxwellforest/onefill/db/gorm/runner/sqlite"
    "github.com/maxwellforest/onefill/test"
)

func TestCreateRunRecordsForSite(t *testing.T) {
    orm, _ := GetORMOrFail()
    defer orm.Close()

    testRR := RunRecord{
        Log:    "This is a test",
        SiteId: "99",
        JobID:  "Yolo",
    }

    rr, err := orm.CreateRunRecordForSite(&testRR.Log, &testRR.SiteId, &testRR.JobID)
    if err != nil {
        t.Errorf("Failed to saved record: %v", err)
    }
    if rr.ID == 0 {
        t.Errorf("Failed to save record")
    }
    if orm.DB.NewRecord(*rr) == true {
        t.Errorf("Failed to save record")
    }
}

func TestGetRunRecordsForSite(t *testing.T) {
    orm, _ := GetORMOrFail()
    defer orm.Close()

    site_id := "99"
    log1 := "log"
    job_id_1 := "100"
    _, _ = orm.CreateRunRecordForSite(&log1, &site_id, &job_id_1)
    job_id_2 := "100"
    log2 := "log"
    _, _ = orm.CreateRunRecordForSite(&log2, &site_id, &job_id_2)

    rr, err := orm.GetRunRecordsForSite(&site_id)
    if err != nil {
        t.Errorf("Didn't expect error, got %v\n", err)
    }
    if len(rr) != (2 + 1) {
        t.Errorf("Expected 2 records, got %d\n", len(rr))
    }
}

func TestCleanUpDB(t *testing.T) {
    _, err := initDB()
    if err != nil {
        t.Errorf("DB Failed to initialize: %s", err.Error())
    }

    exists, err := test.FileExists(sqlite.PathToDB)
    if !exists {
        t.Errorf("Expected file exist, it apparently doesn't")
    }

    CleanUpDB(sqlite.PathToDB)
    exists, err = test.FileExists(sqlite.PathToDB)
    if exists {
        t.Errorf("Expected file not to exist, it apparently does.")
    }
}
```

### `db/gorm/runner/pg` and `/gorm/runner/sqlite`

These were just utility packages that facilitated opening DB-specific connections to the persistence layers. In the future, I'd use pooling to manage the connections. For now, pooling was written into the ORM objects and was determined statically, independent of which database was being used and where the database was (local / remote)

```go
package pg

import (
    "fmt"

    "github.com/jinzhu/gorm"
    _ "github.com/jinzhu/gorm/dialects/postgres"
    _ "github.com/lib/pq"
)

type Config struct {
    Host     string
    User     string
    Password string
    Dbname   string
    SSLMode  string
}

func (c *Config) SerializeGormConn() string {
    return fmt.Sprintf("host=%s user=%s dbname=%s sslmode=%v password=%s", c.Host, c.User, c.Dbname, c.SSLMode, c.Password)
}

// TODO Connection pooling
// TODO Use a channel to open
// TODO Use a channel to free
// TODO Use a switch between cost of freeing connection and creating a new one
func Open(cnf *Config) (*gorm.DB, error) {

    db, err := gorm.Open("postgres", cnf.SerializeGormConn())
    if err != nil {
        return nil, err
    }

    return db, nil
}
```

```go
package sqlite

import (
    "github.com/jinzhu/gorm"
    _ "github.com/jinzhu/gorm/dialects/sqlite"
)

var PathToDB = "runner.db"

func Open(path *string) (*gorm.DB, error) {
    return gorm.Open("sqlite3", *path)
}
```

# Conclusion

Whilst there is a lot of lower-level concerns in the ORM and not a lot of generality, it was fit for purpose. If I was working on a project where the persistence layer was of more importance, I would address the concerns I've raised.
